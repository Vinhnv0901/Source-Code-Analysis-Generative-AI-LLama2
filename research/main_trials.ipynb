{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4b6375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())        #\n",
    "print(torch.cuda.get_device_name(0))    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583f79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6e9d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\d\\\\generative AI\\\\Source-Code-Analysis-Generative-AI-LLama2\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e35109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed26894",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/Vinhnv0901/universal-document-QA_with_Llama2.git\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fffee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb732d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d68f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\ test.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\n# from langchain_pinecone import PineconeVectorStore\\nfrom langchain.vectorstores import Chroma\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\nfrom transformers import pipeline\\nimport torch\\nfrom huggingface_hub import login\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nfrom langchain import HuggingFacePipeline\\napp = Flask(__name__)\\n\\nload_dotenv()\\ntoken = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\\nlogin(token=token)\\n\\n\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\n\\n# Load lại vectordb đã lưu\\npersist_directory = \"db\"\\nvectordb = Chroma(persist_directory=persist_directory,\\n                  embedding_function=embeddings)\\nretriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\n\\n\\n\\nmodel = \"meta-llama/Llama-2-7b-chat-hf\"\\nbnb_config = BitsAndBytesConfig(load_in_4bit=True)\\n\\ntokenizer = AutoTokenizer.from_pretrained(model, token=token)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model,\\n    device_map=\"auto\",\\n    quantization_config=bnb_config,\\n    token=token\\n)\\npipe = pipeline(\"text-generation\",\\n                model=model,\\n                tokenizer= tokenizer,\\n                dtype=torch.bfloat16,\\n                device_map=\"auto\",\\n                max_new_tokens = 512,\\n                do_sample=True,\\n                top_k=30,\\n                num_return_sequences=1,\\n                eos_token_id=tokenizer.eos_token_id\\n                )\\n\\nllm=HuggingFacePipeline(pipeline=pipe, model_kwargs={\\'temperature\\':0.4})\\n\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name=\"universal-document-QA_with_Llama2_Project\",\\n    version=\"0.1.0\",\\n    author=\"Vinh Nguyen\",\\n    author_email=\"vinhvinh673019@gmail.com\",\\n    description=\"A project to generate questions from documents using LLMs and embeddings\",\\n    packages=find_packages(),\\n    install_requires=[],\\n\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom langchain.vectorstores import Chroma\\n\\n\\n\\n\\n\\n\\nextracted_data=load_pdf_file(data='data/')\\ntext_chunks=text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\npersist_directory = 'db'\\n\\n\\nvectordb = Chroma.from_documents(documents=text_chunks,\\n                                 embedding=embeddings,\\n                                 persist_directory=persist_directory)\\n\\nvectordb.persist()\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n   \" test.py\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n#Extract Data From the PDF File\\ndef load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n\\n\\n#Split the Data into Text Chunks\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\n#Download the Embeddings from HuggingFace \\ndef download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name=\\'sentence-transformers/all-MiniLM-L6-v2\\')  #this model return 384 dimensions\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='\\n\\nsystem_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79068445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b669b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_repo\\\\ test.py', 'language': <Language.PYTHON: 'python'>}, page_content='')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3557b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 500,\n",
    "                                                             chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006647bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc024b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\n# from langchain_pinecone import PineconeVectorStore\\nfrom langchain.vectorstores import Chroma\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom transformers import pipeline\\nimport torch\\nfrom huggingface_hub import login\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nfrom langchain import HuggingFacePipeline\\napp = Flask(__name__)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='load_dotenv()\\ntoken = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\\nlogin(token=token)\\n\\n\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\n\\n# Load lại vectordb đã lưu\\npersist_directory = \"db\"\\nvectordb = Chroma(persist_directory=persist_directory,\\n                  embedding_function=embeddings)\\nretriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\n\\n\\n\\nmodel = \"meta-llama/Llama-2-7b-chat-hf\"\\nbnb_config = BitsAndBytesConfig(load_in_4bit=True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='tokenizer = AutoTokenizer.from_pretrained(model, token=token)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model,\\n    device_map=\"auto\",\\n    quantization_config=bnb_config,\\n    token=token\\n)\\npipe = pipeline(\"text-generation\",\\n                model=model,\\n                tokenizer= tokenizer,\\n                dtype=torch.bfloat16,\\n                device_map=\"auto\",\\n                max_new_tokens = 512,\\n                do_sample=True,\\n                top_k=30,'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='num_return_sequences=1,\\n                eos_token_id=tokenizer.eos_token_id\\n                )'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={\\'temperature\\':0.4})\\n\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name=\"universal-document-QA_with_Llama2_Project\",\\n    version=\"0.1.0\",\\n    author=\"Vinh Nguyen\",\\n    author_email=\"vinhvinh673019@gmail.com\",\\n    description=\"A project to generate questions from documents using LLMs and embeddings\",\\n    packages=find_packages(),\\n    install_requires=[],\\n\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom langchain.vectorstores import Chroma\\n\\n\\n\\n\\n\\n\\nextracted_data=load_pdf_file(data='data/')\\ntext_chunks=text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\npersist_directory = 'db'\\n\\n\\nvectordb = Chroma.from_documents(documents=text_chunks,\\n                                 embedding=embeddings,\\n                                 persist_directory=persist_directory)\\n\\nvectordb.persist()\"),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n   \" test.py\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n#Extract Data From the PDF File\\ndef load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n\\n\\n#Split the Data into Text Chunks'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"def text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\n#Download the Embeddings from HuggingFace \\ndef download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')  #this model return 384 dimensions\\n    return embeddings\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aa6a10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "556c4a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d\\generative AI\\Source-Code-Analysis-Generative-AI-LLama2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã login thành công!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load token từ .env\n",
    "load_dotenv()\n",
    "token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "# Login trực tiếp bằng code\n",
    "login(token=token)\n",
    "\n",
    "print(\"✅ Đã login thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0363a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d0b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_15812\\3158813755.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c19fe68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5bdaca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_15812\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccb3c1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "092ce00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "923f9385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_15812\\2412414649.py:2: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29acf4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_15812\\2101274949.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7505c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80f6d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings funtion?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8524b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_15812\\79176006.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "def text_split(extracted_data):\n",
      "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
      "    text_chunks=text_splitter.split_documents(extracted_data)\n",
      "    return text_chunks\n",
      "\n",
      "\n",
      "\n",
      "#Download the Embeddings from HuggingFace \n",
      "def download_hugging_face_embeddings():\n",
      "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')  #this model return 384 dimensions\n",
      "    return embeddings\n",
      "\n",
      "from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "extracted_data=load_pdf_file(data='data/')\n",
      "text_chunks=text_split(extracted_data)\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "persist_directory = 'db'\n",
      "\n",
      "\n",
      "vectordb = Chroma.from_documents(documents=text_chunks,\n",
      "                                 embedding=embeddings,\n",
      "                                 persist_directory=persist_directory)\n",
      "\n",
      "vectordb.persist()\n",
      "\n",
      "load_dotenv()\n",
      "token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
      "login(token=token)\n",
      "\n",
      "\n",
      "\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "\n",
      "\n",
      "\n",
      "# Load lại vectordb đã lưu\n",
      "persist_directory = \"db\"\n",
      "vectordb = Chroma(persist_directory=persist_directory,\n",
      "                  embedding_function=embeddings)\n",
      "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
      "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
      "\n",
      "from flask import Flask, render_template, jsonify, request\n",
      "from src.helper import download_hugging_face_embeddings\n",
      "# from langchain_pinecone import PineconeVectorStore\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain_openai import OpenAI\n",
      "from langchain.chains import create_retrieval_chain\n",
      "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from dotenv import load_dotenv\n",
      "from src.prompt import *\n",
      "import os\n",
      "\n",
      "import os\n",
      "from transformers import pipeline\n",
      "import torch\n",
      "from huggingface_hub import login\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
      "from langchain import HuggingFacePipeline\n",
      "app = Flask(__name__)\n",
      "\n",
      "def index():\n",
      "    return render_template('chat.html')\n",
      "\n",
      "\n",
      "@app.route(\"/get\", methods=[\"GET\", \"POST\"])\n",
      "def chat():\n",
      "    msg = request.form[\"msg\"]\n",
      "    input = msg\n",
      "    print(input)\n",
      "    response = rag_chain.invoke({\"input\": msg})\n",
      "    print(\"Response : \", response[\"answer\"])\n",
      "    return str(response[\"answer\"])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n",
      "\n",
      "system_prompt = (\n",
      "    \"You are an assistant for question-answering tasks. \"\n",
      "    \"Use the following pieces of retrieved context to answer \"\n",
      "    \"the question. If you don't know the answer, say that you \"\n",
      "    \"don't know. Use three sentences maximum and keep the \"\n",
      "    \"answer concise.\"\n",
      "    \"\\n\\n\"\n",
      "    \"{context}\"\n",
      ")\n",
      "\n",
      "if filedir !=\"\":\n",
      "        os.makedirs(filedir, exist_ok=True)\n",
      "        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\n",
      "\n",
      "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
      "        with open(filepath, \"w\") as f:\n",
      "            pass\n",
      "            logging.info(f\"Creating empty file: {filepath}\")\n",
      "\n",
      "\n",
      "    else:\n",
      "        logging.info(f\"{filename} is already exists\")\n",
      "\n",
      "Question: what is download_hugging_face_embeddings funtion?\n",
      "Helpful Answer: download_hugging_face_embeddings is a function that downloads the pre-trained Hugging Face embeddings from the Hugging Face Hub API. The function takes no arguments and returns the downloaded embeddings.\n",
      "\n",
      "Unhelpful Answer: I don't know the answer to your question, I'm just an AI bot trained to generate random text.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04844e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "def text_split(extracted_data):\n",
      "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
      "    text_chunks=text_splitter.split_documents(extracted_data)\n",
      "    return text_chunks\n",
      "\n",
      "\n",
      "\n",
      "#Download the Embeddings from HuggingFace \n",
      "def download_hugging_face_embeddings():\n",
      "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')  #this model return 384 dimensions\n",
      "    return embeddings\n",
      "\n",
      "system_prompt = (\n",
      "    \"You are an assistant for question-answering tasks. \"\n",
      "    \"Use the following pieces of retrieved context to answer \"\n",
      "    \"the question. If you don't know the answer, say that you \"\n",
      "    \"don't know. Use three sentences maximum and keep the \"\n",
      "    \"answer concise.\"\n",
      "    \"\\n\\n\"\n",
      "    \"{context}\"\n",
      ")\n",
      "\n",
      "def index():\n",
      "    return render_template('chat.html')\n",
      "\n",
      "\n",
      "@app.route(\"/get\", methods=[\"GET\", \"POST\"])\n",
      "def chat():\n",
      "    msg = request.form[\"msg\"]\n",
      "    input = msg\n",
      "    print(input)\n",
      "    response = rag_chain.invoke({\"input\": msg})\n",
      "    print(\"Response : \", response[\"answer\"])\n",
      "    return str(response[\"answer\"])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "\n",
      "setup(\n",
      "    name=\"universal-document-QA_with_Llama2_Project\",\n",
      "    version=\"0.1.0\",\n",
      "    author=\"Vinh Nguyen\",\n",
      "    author_email=\"vinhvinh673019@gmail.com\",\n",
      "    description=\"A project to generate questions from documents using LLMs and embeddings\",\n",
      "    packages=find_packages(),\n",
      "    install_requires=[],\n",
      "\n",
      ")\n",
      "\n",
      "load_dotenv()\n",
      "token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
      "login(token=token)\n",
      "\n",
      "\n",
      "\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "\n",
      "\n",
      "\n",
      "# Load lại vectordb đã lưu\n",
      "persist_directory = \"db\"\n",
      "vectordb = Chroma(persist_directory=persist_directory,\n",
      "                  embedding_function=embeddings)\n",
      "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
      "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
      "\n",
      "num_return_sequences=1,\n",
      "                eos_token_id=tokenizer.eos_token_id\n",
      "                )\n",
      "\n",
      "import os\n",
      "from transformers import pipeline\n",
      "import torch\n",
      "from huggingface_hub import login\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
      "from langchain import HuggingFacePipeline\n",
      "app = Flask(__name__)\n",
      "\n",
      "if filedir !=\"\":\n",
      "        os.makedirs(filedir, exist_ok=True)\n",
      "        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\n",
      "\n",
      "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
      "        with open(filepath, \"w\") as f:\n",
      "            pass\n",
      "            logging.info(f\"Creating empty file: {filepath}\")\n",
      "\n",
      "\n",
      "    else:\n",
      "        logging.info(f\"{filename} is already exists\")\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: what is download_hugging_face_embeddings funtion?\n",
      "AI: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "def text_split(extracted_data):\n",
      "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
      "    text_chunks=text_splitter.split_documents(extracted_data)\n",
      "    return text_chunks\n",
      "\n",
      "\n",
      "\n",
      "#Download the Embeddings from HuggingFace \n",
      "def download_hugging_face_embeddings():\n",
      "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')  #this model return 384 dimensions\n",
      "    return embeddings\n",
      "\n",
      "from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "extracted_data=load_pdf_file(data='data/')\n",
      "text_chunks=text_split(extracted_data)\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "persist_directory = 'db'\n",
      "\n",
      "\n",
      "vectordb = Chroma.from_documents(documents=text_chunks,\n",
      "                                 embedding=embeddings,\n",
      "                                 persist_directory=persist_directory)\n",
      "\n",
      "vectordb.persist()\n",
      "\n",
      "load_dotenv()\n",
      "token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
      "login(token=token)\n",
      "\n",
      "\n",
      "\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "\n",
      "\n",
      "\n",
      "# Load lại vectordb đã lưu\n",
      "persist_directory = \"db\"\n",
      "vectordb = Chroma(persist_directory=persist_directory,\n",
      "                  embedding_function=embeddings)\n",
      "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
      "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
      "\n",
      "from flask import Flask, render_template, jsonify, request\n",
      "from src.helper import download_hugging_face_embeddings\n",
      "# from langchain_pinecone import PineconeVectorStore\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain_openai import OpenAI\n",
      "from langchain.chains import create_retrieval_chain\n",
      "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from dotenv import load_dotenv\n",
      "from src.prompt import *\n",
      "import os\n",
      "\n",
      "import os\n",
      "from transformers import pipeline\n",
      "import torch\n",
      "from huggingface_hub import login\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
      "from langchain import HuggingFacePipeline\n",
      "app = Flask(__name__)\n",
      "\n",
      "def index():\n",
      "    return render_template('chat.html')\n",
      "\n",
      "\n",
      "@app.route(\"/get\", methods=[\"GET\", \"POST\"])\n",
      "def chat():\n",
      "    msg = request.form[\"msg\"]\n",
      "    input = msg\n",
      "    print(input)\n",
      "    response = rag_chain.invoke({\"input\": msg})\n",
      "    print(\"Response : \", response[\"answer\"])\n",
      "    return str(response[\"answer\"])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n",
      "\n",
      "system_prompt = (\n",
      "    \"You are an assistant for question-answering tasks. \"\n",
      "    \"Use the following pieces of retrieved context to answer \"\n",
      "    \"the question. If you don't know the answer, say that you \"\n",
      "    \"don't know. Use three sentences maximum and keep the \"\n",
      "    \"answer concise.\"\n",
      "    \"\\n\\n\"\n",
      "    \"{context}\"\n",
      ")\n",
      "\n",
      "if filedir !=\"\":\n",
      "        os.makedirs(filedir, exist_ok=True)\n",
      "        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\n",
      "\n",
      "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
      "        with open(filepath, \"w\") as f:\n",
      "            pass\n",
      "            logging.info(f\"Creating empty file: {filepath}\")\n",
      "\n",
      "\n",
      "    else:\n",
      "        logging.info(f\"{filename} is already exists\")\n",
      "\n",
      "Question: what is download_hugging_face_embeddings funtion?\n",
      "Helpful Answer: download_hugging_face_embeddings is a function that downloads the pre-trained Hugging Face embeddings from the Hugging Face Hub API. The function takes no arguments and returns the downloaded embeddings.\n",
      "\n",
      "Unhelpful Answer: I don't know the answer to your question, I'm just an AI bot trained to generate random text.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good. The human asks what download_hugging_face_embeddings function does, and the AI explains that it downloads pre-trained Hugging Face embeddings from the Hugging Face Hub API.\n",
      "END OF EXAMPLE\n",
      "Follow Up Input: what is load_pdf_file funtion?\n",
      "Standalone question: What is load_pdf_file function?\n",
      "Helpful Answer: load_pdf_file is a function that loads a PDF file into a Python dictionary. The function takes a filename as an argument and returns the loaded dictionary.\n",
      "\n",
      "Unhelpful Answer: I don't know the answer to your question, I'm just an AI bot trained to generate random text.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good. The human asks what load_pdf_file function does, and the AI explains that it loads a PDF file into a Python dictionary.\n",
      "END OF EXAMPLE\n"
     ]
    }
   ],
   "source": [
    "question = \"what is load_pdf_file funtion?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1dd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
